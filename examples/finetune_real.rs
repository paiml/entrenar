//! Real End-to-End Fine-Tuning for Rust Test Generation
//!
//! This example loads actual model weights from SafeTensors,
//! creates a real training corpus, and performs actual fine-tuning
//! with real forward passes through the transformer.
//!
//! ## CUDA Acceleration (SPEC-FT-001 v3.3.0)
//!
//! When compiled with `--features cuda`, this example automatically uses
//! GPU-accelerated training via `CudaTrainer`:
//!
//! ```bash
//! cargo run --example finetune_real --release --features cuda
//! ```
//!
//! ## TUI Monitoring (SPEC-FT-001 Section 10)
//!
//! Run training and monitoring in separate terminals:
//!
//! ```bash
//! # Terminal 1: Start training (Producer)
//! cargo run --example finetune_real --release --features cuda,nvml -- \
//!     --output ./experiments/finetune-real
//!
//! # Terminal 2: Attach TUI Monitor (Consumer)
//! cargo run --example finetune_real --features nvml -- \
//!     --monitor --experiment ./experiments/finetune-real
//! ```
//!
//! ## Headless Mode for CI/CD (SPEC-FT-001 Section 10.8)
//!
//! For CI/CD pipelines and AI agents, use headless mode:
//!
//! ```bash
//! # JSON output to stdout (machine-readable, default)
//! cargo run --example finetune_real -- \
//!     --headless --format json --experiment ./experiments/finetune-real
//!
//! # Text output to stdout (human-readable logs)
//! cargo run --example finetune_real -- \
//!     --headless --format text --experiment ./experiments/finetune-real
//!
//! # Write output to file instead of stdout
//! cargo run --example finetune_real -- \
//!     --headless --format json --output-file ./training.jsonl --experiment ./experiments/finetune-real
//! ```
//!
//! Prerequisites:
//!   apr pull Qwen/Qwen2.5-Coder-0.5B-Instruct
//!
//! Run with:
//!   cargo run --example finetune_real --release

use clap::Parser;
#[allow(unused_imports)]
use entrenar::autograd::{backward, cuda_training_available, matmul, CudaTrainer};
use entrenar::finetune::{
    ComputeDevice, DeviceInfo, PopperianQA, QAGrade, ReproducibilityConfig, TestEvaluator,
    TestGenCorpus, TestGenSample,
};
use entrenar::hf_pipeline::{FetchOptions, HfModelFetcher};
// LoRA types available but not used directly in this experiment
#[allow(unused_imports)]
use entrenar::lora::{LoRALayer, QLoRALayer};
use entrenar::monitor::gpu::GpuMonitor;
use entrenar::monitor::tui::app::{TrainingStateWriter, TuiMonitor, TuiMonitorConfig};
use entrenar::monitor::tui::headless::{HeadlessMonitor, OutputFormat};
use entrenar::monitor::tui::state::{GpuProcessInfo, GpuTelemetry, SamplePeek};
use entrenar::optim::{AdamW, CosineAnnealingLR, LRScheduler, Optimizer};
use entrenar::tokenizer::HfTokenizer;
use entrenar::train::{CausalLMLoss, LossFn};
use entrenar::transformer::{
    load_safetensors_weights, Architecture, LoRAProjection, Transformer, TransformerConfig,
};
use entrenar::Tensor;
use std::fs;
use std::path::Path;
use std::time::Instant;

/// Real End-to-End Fine-Tuning for Rust Test Generation
#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// Output directory for experiment artifacts and state
    #[arg(short, long, default_value = "./experiments/finetune-real")]
    output: String,

    /// Run in TUI monitor mode (consumer - watches training state)
    #[arg(long, default_value_t = false)]
    monitor: bool,

    /// Run in headless mode (CI/CD - no TUI, output to stdout)
    #[arg(long, default_value_t = false)]
    headless: bool,

    /// Output format for headless mode (json or text)
    #[arg(long, default_value = "json")]
    format: String,

    /// Output file for headless mode (default: stdout)
    #[arg(long)]
    output_file: Option<String>,

    /// Experiment directory to monitor (only with --monitor or --headless)
    #[arg(long)]
    experiment: Option<String>,

    /// TUI refresh interval in milliseconds
    #[arg(long, default_value_t = 500)]
    refresh_ms: u64,
}

/// CUDA-accelerated training state (SPEC-FT-001 v3.3.0)
/// Manages GPU buffers for high-performance training
#[cfg(feature = "cuda")]
struct CudaTrainingState {
    trainer: CudaTrainer,
    /// Weights on GPU: (hidden_size Ã— vocab_size)
    weights_gpu: trueno_gpu::driver::GpuBuffer<f32>,
    /// Weight gradients on GPU
    grads_gpu: trueno_gpu::driver::GpuBuffer<f32>,
    /// Adam first moment (m)
    m_state: trueno_gpu::driver::GpuBuffer<f32>,
    /// Adam second moment (v)
    v_state: trueno_gpu::driver::GpuBuffer<f32>,
    /// Dimensions
    hidden_size: usize,
    vocab_size: usize,
}

#[cfg(feature = "cuda")]
impl CudaTrainingState {
    /// Initialize CUDA training state with weights
    fn new(weights: &[f32], hidden_size: usize, vocab_size: usize) -> Option<Self> {
        let trainer = CudaTrainer::new().ok()?;
        let weights_gpu = trainer.upload(weights).ok()?;
        let grads_gpu = trainer.zeros(weights.len()).ok()?;
        let m_state = trainer.zeros(weights.len()).ok()?;
        let v_state = trainer.zeros(weights.len()).ok()?;

        Some(Self {
            trainer,
            weights_gpu,
            grads_gpu,
            m_state,
            v_state,
            hidden_size,
            vocab_size,
        })
    }

    /// Forward pass: logits = hidden @ weights
    fn forward(&self, hidden: &[f32], seq_len: usize) -> Option<Vec<f32>> {
        let hidden_gpu = self.trainer.upload(hidden).ok()?;
        let mut logits_gpu = self.trainer.zeros(seq_len * self.vocab_size).ok()?;

        self.trainer
            .matmul_forward(
                &hidden_gpu,
                &self.weights_gpu,
                &mut logits_gpu,
                seq_len as u32,
                self.hidden_size as u32,
                self.vocab_size as u32,
            )
            .ok()?;

        self.trainer.download(&logits_gpu).ok()
    }

    /// Compute gradients: grad_weights = hidden^T @ grad_logits
    fn backward(&mut self, hidden: &[f32], grad_logits: &[f32], seq_len: usize) -> Option<()> {
        let hidden_gpu = self.trainer.upload(hidden).ok()?;
        let grad_logits_gpu = self.trainer.upload(grad_logits).ok()?;
        let mut grad_hidden_gpu = self.trainer.zeros(hidden.len()).ok()?;

        self.trainer
            .matmul_backward(
                &hidden_gpu,
                &self.weights_gpu,
                &grad_logits_gpu,
                &mut grad_hidden_gpu,
                &mut self.grads_gpu,
                seq_len as u32,
                self.hidden_size as u32,
                self.vocab_size as u32,
            )
            .ok()?;

        Some(())
    }

    /// AdamW optimizer step on GPU
    fn adamw_step(
        &mut self,
        lr: f32,
        beta1: f32,
        beta2: f32,
        eps: f32,
        weight_decay: f32,
    ) -> Option<()> {
        self.trainer
            .adamw_step(
                &mut self.weights_gpu,
                &self.grads_gpu,
                &mut self.m_state,
                &mut self.v_state,
                lr,
                beta1,
                beta2,
                eps,
                weight_decay,
            )
            .ok()
    }

    /// Download current weights from GPU
    fn download_weights(&self) -> Option<Vec<f32>> {
        self.trainer.download(&self.weights_gpu).ok()
    }

    /// Get GPU name for logging
    fn device_name(&self) -> String {
        self.trainer.device_name()
    }
}

/// LoRA adapters for attention projections
/// Each layer has adapters for Q, K, V, O projections
#[allow(dead_code)] // Scaffold for future LoRA implementation
struct AttentionLoRAAdapters {
    /// LoRA A matrices for each layer's Q projection [num_layers][rank * hidden_size]
    q_lora_a: Vec<Tensor>,
    q_lora_b: Vec<Tensor>,
    /// LoRA A matrices for each layer's K projection
    k_lora_a: Vec<Tensor>,
    k_lora_b: Vec<Tensor>,
    /// LoRA A matrices for each layer's V projection
    v_lora_a: Vec<Tensor>,
    v_lora_b: Vec<Tensor>,
    /// LoRA A matrices for each layer's O projection
    o_lora_a: Vec<Tensor>,
    o_lora_b: Vec<Tensor>,
    /// Configuration
    num_layers: usize,
    hidden_size: usize,
    rank: usize,
    scale: f32,
}

#[allow(dead_code)]
impl AttentionLoRAAdapters {
    /// Create LoRA adapters for all attention projections across all layers
    fn new(num_layers: usize, hidden_size: usize, rank: usize, alpha: f32) -> Self {
        let scale = alpha / rank as f32;

        // Initialize matrices for the computation: h' = h + scale * (h @ A) @ B
        // A is (h Ã— r), B is (r Ã— h)
        // - A projects h-dim to r-dim (down-projection)
        // - B projects r-dim back to h-dim (up-projection)

        // A: (hidden_size Ã— rank), initialized with small values
        let init_a = |layer: usize, proj: usize| -> Tensor {
            let data: Vec<f32> = (0..hidden_size * rank)
                .map(|i| {
                    let seed = (layer * 4 + proj) * 1000 + i;
                    ((seed as f32 * 0.123).sin() * 0.01)
                })
                .collect();
            Tensor::from_vec(data, true)
        };

        // B: (rank Ã— hidden_size), initialized with small values
        // (Standard LoRA uses zeros for B, but we use small values to show effect immediately)
        let init_b = |layer: usize, proj: usize| -> Tensor {
            let data: Vec<f32> = (0..rank * hidden_size)
                .map(|i| {
                    let seed = (layer * 4 + proj + 100) * 1000 + i;
                    ((seed as f32 * 0.234).sin() * 0.005)
                })
                .collect();
            Tensor::from_vec(data, true)
        };

        Self {
            q_lora_a: (0..num_layers).map(|l| init_a(l, 0)).collect(),
            q_lora_b: (0..num_layers).map(|l| init_b(l, 0)).collect(),
            k_lora_a: (0..num_layers).map(|l| init_a(l, 1)).collect(),
            k_lora_b: (0..num_layers).map(|l| init_b(l, 1)).collect(),
            v_lora_a: (0..num_layers).map(|l| init_a(l, 2)).collect(),
            v_lora_b: (0..num_layers).map(|l| init_b(l, 2)).collect(),
            o_lora_a: (0..num_layers).map(|l| init_a(l, 3)).collect(),
            o_lora_b: (0..num_layers).map(|l| init_b(l, 3)).collect(),
            num_layers,
            hidden_size,
            rank,
            scale,
        }
    }

    /// Get all trainable LoRA A matrices as a flat vector
    fn lora_a_params_mut(&mut self) -> Vec<&mut Tensor> {
        let mut params = Vec::new();
        params.extend(self.q_lora_a.iter_mut());
        params.extend(self.k_lora_a.iter_mut());
        params.extend(self.v_lora_a.iter_mut());
        params.extend(self.o_lora_a.iter_mut());
        params
    }

    /// Get all trainable LoRA B matrices as a flat vector
    fn lora_b_params_mut(&mut self) -> Vec<&mut Tensor> {
        let mut params = Vec::new();
        params.extend(self.q_lora_b.iter_mut());
        params.extend(self.k_lora_b.iter_mut());
        params.extend(self.v_lora_b.iter_mut());
        params.extend(self.o_lora_b.iter_mut());
        params
    }

    /// Count total trainable parameters
    fn total_params(&self) -> usize {
        // 4 projections * 2 matrices (A,B) * num_layers * (rank * hidden_size)
        4 * 2 * self.num_layers * self.rank * self.hidden_size
    }

    /// Apply LoRA transformation using actual matmul operations with gradient flow
    /// h' = h + scale * (h @ A) @ B
    /// Where A is (h Ã— r) and B is (r Ã— h)
    fn apply_lora_transform(&self, hidden: &Tensor, seq_len: usize) -> Tensor {
        let h = self.hidden_size;
        let r = self.rank;

        // Apply LoRA from a single representative layer (layer 0) to keep it efficient
        // In a full implementation, this would be integrated into each layer's forward pass
        let layer = 0;

        // Use O projection LoRA (output projection has strongest effect on hidden states)
        let a = &self.o_lora_a[layer];
        let b = &self.o_lora_b[layer];

        // LoRA: h' = h + scale * (h @ A) @ B
        // A is (h Ã— r), B is (r Ã— h)

        // Step 1: h @ A: (seq Ã— h) @ (h Ã— r) = (seq Ã— r)
        let intermediate = matmul(hidden, a, seq_len, h, r);

        // Step 2: intermediate @ B: (seq Ã— r) @ (r Ã— h) = (seq Ã— h)
        let lora_output = matmul(&intermediate, b, seq_len, r, h);

        // Step 3: Scale and add to hidden states
        // h' = h + scale * lora_output
        let result_data: Vec<f32> = hidden
            .data()
            .iter()
            .zip(lora_output.data().iter())
            .map(|(&h_val, &l_val)| h_val + self.scale * l_val)
            .collect();

        Tensor::from_vec(result_data, true)
    }

    /// Apply full LoRA matmul transformation (more accurate but slower)
    fn apply_lora_matmul(&self, hidden: &Tensor, seq_len: usize, layer_idx: usize) -> Tensor {
        let h = self.hidden_size;
        let r = self.rank;

        // Aggregate contribution from all 4 projections
        let mut total_lora = vec![0.0f32; seq_len * h];

        for (a, b) in [
            (&self.q_lora_a[layer_idx], &self.q_lora_b[layer_idx]),
            (&self.k_lora_a[layer_idx], &self.k_lora_b[layer_idx]),
            (&self.v_lora_a[layer_idx], &self.v_lora_b[layer_idx]),
            (&self.o_lora_a[layer_idx], &self.o_lora_b[layer_idx]),
        ] {
            // x @ A^T: (seq Ã— h) @ (h Ã— r) = (seq Ã— r)
            // We need A transposed, but A is stored as (r Ã— h), so A^T is (h Ã— r)
            // Actually matmul(x, A, seq, h, r) treats A as (h Ã— r) in row-major
            // But A is stored as (r Ã— h), so we need to transpose
            let a_t = transpose_matrix(a.data().as_slice().unwrap(), r, h); // (r Ã— h) -> (h Ã— r)
            let a_t_tensor = Tensor::from_vec(a_t, false);
            let intermediate = matmul(hidden, &a_t_tensor, seq_len, h, r);

            // intermediate @ B^T: (seq Ã— r) @ (r Ã— h) = (seq Ã— h)
            // B is (h Ã— r), so B^T is (r Ã— h)
            let b_t = transpose_matrix(b.data().as_slice().unwrap(), h, r); // (h Ã— r) -> (r Ã— h)
            let b_t_tensor = Tensor::from_vec(b_t, false);
            let lora_out = matmul(&intermediate, &b_t_tensor, seq_len, r, h);

            // Add scaled contribution
            for (i, val) in total_lora.iter_mut().enumerate() {
                *val += self.scale * lora_out.data()[i];
            }
        }

        // h' = h + total_lora
        let result_data: Vec<f32> = hidden
            .data()
            .iter()
            .zip(total_lora.iter())
            .map(|(&h, &l)| h + l)
            .collect();

        Tensor::from_vec(result_data, true)
    }
}

/// Transpose a row-major matrix (rows Ã— cols) to (cols Ã— rows)
#[allow(dead_code)]
fn transpose_matrix(data: &[f32], rows: usize, cols: usize) -> Vec<f32> {
    let mut transposed = vec![0.0f32; rows * cols];
    for r in 0..rows {
        for c in 0..cols {
            transposed[c * rows + r] = data[r * cols + c];
        }
    }
    transposed
}

/// Truncate a string to a maximum length, adding "..." if truncated (ENT-142)
fn truncate_str(s: &str, max_len: usize) -> String {
    // Replace newlines with spaces for single-line display
    let single_line: String = s.chars().map(|c| if c == '\n' { ' ' } else { c }).collect();
    if single_line.len() <= max_len {
        single_line
    } else {
        format!("{}...", &single_line[..max_len.saturating_sub(3)])
    }
}

/// Generate test code using the trained LoRA model
/// Uses greedy decoding (argmax) for simplicity
fn generate_tests(
    transformer: &Transformer,
    lora_lm_head: &LoRAProjection,
    tokenizer: &HfTokenizer,
    function_code: &str,
    max_new_tokens: usize,
    _hidden_size: usize,
) -> String {
    // Create prompt: function + test generation instruction
    let prompt = format!(
        "{}\n\n// Generate comprehensive tests for the above function:\n#[cfg(test)]\nmod tests {{\n    use super::*;\n\n    #[test]\n    fn ",
        function_code
    );

    // Tokenize prompt
    let mut token_ids = tokenizer.encode(&prompt);
    let eos_token = tokenizer.eos_id().unwrap_or(128247);

    // Generate tokens one at a time
    for _ in 0..max_new_tokens {
        // Get hidden states from transformer
        let hidden_states = forward_hidden(transformer, &token_ids);
        let seq_len = token_ids.len();

        // Apply LoRA LM head to get logits for last position
        let logits = lora_lm_head.forward(&hidden_states, seq_len);

        // Get logits for last token position
        // logits shape: (seq_len * vocab_size) but we only need last position
        let logits_vec = logits.data().to_vec();
        let vocab_size = logits_vec.len() / seq_len;
        let last_pos_start = (seq_len - 1) * vocab_size;
        let last_pos_end = last_pos_start + vocab_size;

        // Greedy decode: pick token with highest logit
        let next_token = logits_vec[last_pos_start..last_pos_end]
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map(|(idx, _)| idx as u32)
            .unwrap_or(eos_token);

        // Stop if EOS
        if next_token == eos_token {
            break;
        }

        // Add token to sequence
        token_ids.push(next_token);

        // Stop if we generate closing brace (end of test module)
        if tokenizer.decode(&[next_token]).contains('}') {
            // Check if we've generated enough closing braces
            let decoded = tokenizer.decode(&token_ids);
            let open_braces = decoded.matches('{').count();
            let close_braces = decoded.matches('}').count();
            if close_braces >= open_braces {
                break;
            }
        }
    }

    // Decode back to text
    tokenizer.decode(&token_ids)
}

const IS_PRIME_FN: &str = r#"/// Checks if a number is prime
pub fn is_prime(n: u64) -> bool {
    if n < 2 { return false; }
    if n == 2 { return true; }
    if n % 2 == 0 { return false; }
    let sqrt_n = (n as f64).sqrt() as u64;
    for i in (3..=sqrt_n).step_by(2) {
        if n % i == 0 { return false; }
    }
    true
}"#;

const IS_PRIME_TESTS: &str = r#"#[test]
fn test_is_prime_small_primes() {
    assert!(is_prime(2));
    assert!(is_prime(3));
    assert!(is_prime(5));
    assert!(is_prime(7));
}

#[test]
fn test_is_prime_composites() {
    assert!(!is_prime(4));
    assert!(!is_prime(6));
    assert!(!is_prime(9));
}

#[test]
fn test_is_prime_edge_cases() {
    assert!(!is_prime(0));
    assert!(!is_prime(1));
}"#;

const IS_PRIME_PROPS: &str = r#"proptest! {
    #[test]
    fn prop_prime_greater_than_one(n in 2u64..1000) {
        if is_prime(n) {
            prop_assert!(n >= 2);
        }
    }
}"#;

fn sample_is_prime() -> TestGenSample {
    TestGenSample {
        function: IS_PRIME_FN.into(),
        unit_tests: IS_PRIME_TESTS.into(),
        property_tests: Some(IS_PRIME_PROPS.into()),
        metadata: Default::default(),
    }
}

const BINARY_SEARCH_FN: &str = r#"/// Binary search in a sorted slice
pub fn binary_search<T: Ord>(arr: &[T], target: &T) -> Option<usize> {
    let mut left = 0;
    let mut right = arr.len();
    while left < right {
        let mid = left + (right - left) / 2;
        match arr[mid].cmp(target) {
            std::cmp::Ordering::Equal => return Some(mid),
            std::cmp::Ordering::Less => left = mid + 1,
            std::cmp::Ordering::Greater => right = mid,
        }
    }
    None
}"#;

const BINARY_SEARCH_TESTS: &str = r#"#[test]
fn test_binary_search_found() {
    let arr = vec![1, 2, 3, 4, 5];
    assert_eq!(binary_search(&arr, &3), Some(2));
}

#[test]
fn test_binary_search_not_found() {
    let arr = vec![1, 2, 4, 5];
    assert_eq!(binary_search(&arr, &3), None);
}

#[test]
fn test_binary_search_empty() {
    let arr: Vec<i32> = vec![];
    assert_eq!(binary_search(&arr, &1), None);
}"#;

const BINARY_SEARCH_PROPS: &str = r#"proptest! {
    #[test]
    fn prop_binary_search_finds_existing(arr in prop::collection::vec(0i32..100, 1..50)) {
        let mut sorted = arr.clone();
        sorted.sort();
        if let Some(&elem) = sorted.first() {
            prop_assert!(binary_search(&sorted, &elem).is_some());
        }
    }
}"#;

fn sample_binary_search() -> TestGenSample {
    TestGenSample {
        function: BINARY_SEARCH_FN.into(),
        unit_tests: BINARY_SEARCH_TESTS.into(),
        property_tests: Some(BINARY_SEARCH_PROPS.into()),
        metadata: Default::default(),
    }
}

const REVERSE_STRING_FN: &str = r#"/// Reverses a string
pub fn reverse_string(s: &str) -> String {
    s.chars().rev().collect()
}"#;

const REVERSE_STRING_TESTS: &str = r#"#[test]
fn test_reverse_string() {
    assert_eq!(reverse_string("hello"), "olleh");
    assert_eq!(reverse_string(""), "");
    assert_eq!(reverse_string("a"), "a");
}

#[test]
fn test_reverse_unicode() {
    assert_eq!(reverse_string("hÃ©llo"), "ollÃ©h");
}"#;

const REVERSE_STRING_PROPS: &str = r#"proptest! {
    #[test]
    fn prop_reverse_twice_is_identity(s in ".*") {
        prop_assert_eq!(reverse_string(&reverse_string(&s)), s);
    }
}"#;

fn sample_reverse_string() -> TestGenSample {
    TestGenSample {
        function: REVERSE_STRING_FN.into(),
        unit_tests: REVERSE_STRING_TESTS.into(),
        property_tests: Some(REVERSE_STRING_PROPS.into()),
        metadata: Default::default(),
    }
}

const FACTORIAL_FN: &str = r#"/// Calculates factorial
pub fn factorial(n: u64) -> u64 {
    match n {
        0 | 1 => 1,
        _ => n * factorial(n - 1),
    }
}"#;

const FACTORIAL_TESTS: &str = r#"#[test]
fn test_factorial_base_cases() {
    assert_eq!(factorial(0), 1);
    assert_eq!(factorial(1), 1);
}

#[test]
fn test_factorial_small() {
    assert_eq!(factorial(5), 120);
    assert_eq!(factorial(10), 3628800);
}"#;

fn sample_factorial() -> TestGenSample {
    TestGenSample {
        function: FACTORIAL_FN.into(),
        unit_tests: FACTORIAL_TESTS.into(),
        property_tests: None,
        metadata: Default::default(),
    }
}

const FLATTEN_FN: &str = r#"/// Flattens a nested vector
pub fn flatten<T: Clone>(nested: Vec<Vec<T>>) -> Vec<T> {
    nested.into_iter().flatten().collect()
}"#;

const FLATTEN_TESTS: &str = r#"#[test]
fn test_flatten() {
    let nested = vec![vec![1, 2], vec![3, 4]];
    assert_eq!(flatten(nested), vec![1, 2, 3, 4]);
}

#[test]
fn test_flatten_empty() {
    let nested: Vec<Vec<i32>> = vec![];
    assert_eq!(flatten(nested), Vec::<i32>::new());
}"#;

fn sample_flatten() -> TestGenSample {
    TestGenSample {
        function: FLATTEN_FN.into(),
        unit_tests: FLATTEN_TESTS.into(),
        property_tests: None,
        metadata: Default::default(),
    }
}

/// Build the core hand-written training samples with property tests
fn build_core_samples() -> Vec<TestGenSample> {
    vec![
        sample_is_prime(),
        sample_binary_search(),
        sample_reverse_string(),
        sample_factorial(),
        sample_flatten(),
    ]
}

/// Split samples into train/val/test corpus with 80/10/10 ratio
fn split_into_corpus(all_samples: Vec<TestGenSample>) -> TestGenCorpus {
    let train_size = (all_samples.len() as f32 * 0.8) as usize;
    let val_size = (all_samples.len() as f32 * 0.1) as usize;

    let mut corpus = TestGenCorpus::new();
    for (i, sample) in all_samples.into_iter().enumerate() {
        if i < train_size {
            corpus.train.push(sample);
        } else if i < train_size + val_size {
            corpus.validation.push(sample);
        } else {
            corpus.test.push(sample);
        }
    }
    corpus
}

/// Create a real corpus of Rust functions and their tests
fn create_real_corpus() -> TestGenCorpus {
    println!("ğŸ“š Creating real Rust test generation corpus...");

    let mut all_samples = build_core_samples();
    // Generate additional samples for better GPU utilization (ENT-136)
    // Balance: enough samples for good GPU util, but not too many for <30s LoRA target
    all_samples.extend(generate_additional_samples(15)); // 20 total samples

    let corpus = split_into_corpus(all_samples);

    let stats = corpus.stats();
    println!(
        "   âœ“ Created corpus: {} train, {} val, {} test samples",
        stats.train_samples, stats.validation_samples, stats.test_samples
    );
    println!(
        "   âœ“ Proptest coverage: {:.0}%",
        (stats.with_proptest as f32 / stats.total_samples as f32) * 100.0
    );

    corpus
}

/// Generate additional training samples for GPU utilization (ENT-136)
fn generate_additional_samples(count: usize) -> Vec<TestGenSample> {
    let templates = vec![
        // Math operations
        (
            "/// Computes the sum of numbers from 1 to n\npub fn sum_to_n(n: u64) -> u64 {\n    (1..=n).sum()\n}",
            "#[test]\nfn test_sum_to_n() {\n    assert_eq!(sum_to_n(5), 15);\n    assert_eq!(sum_to_n(10), 55);\n    assert_eq!(sum_to_n(0), 0);\n}",
        ),
        (
            "/// Computes the product of numbers from 1 to n\npub fn product_to_n(n: u64) -> u64 {\n    (1..=n).product()\n}",
            "#[test]\nfn test_product_to_n() {\n    assert_eq!(product_to_n(5), 120);\n    assert_eq!(product_to_n(1), 1);\n}",
        ),
        (
            "/// Computes the nth Fibonacci number\npub fn fibonacci(n: u64) -> u64 {\n    match n {\n        0 => 0,\n        1 => 1,\n        _ => fibonacci(n - 1) + fibonacci(n - 2),\n    }\n}",
            "#[test]\nfn test_fibonacci() {\n    assert_eq!(fibonacci(0), 0);\n    assert_eq!(fibonacci(1), 1);\n    assert_eq!(fibonacci(10), 55);\n}",
        ),
        (
            "/// Checks if a number is even\npub fn is_even(n: i64) -> bool {\n    n % 2 == 0\n}",
            "#[test]\nfn test_is_even() {\n    assert!(is_even(2));\n    assert!(is_even(0));\n    assert!(!is_even(3));\n}",
        ),
        (
            "/// Returns the absolute value\npub fn abs(n: i64) -> i64 {\n    if n < 0 { -n } else { n }\n}",
            "#[test]\nfn test_abs() {\n    assert_eq!(abs(-5), 5);\n    assert_eq!(abs(5), 5);\n    assert_eq!(abs(0), 0);\n}",
        ),
        // String operations
        (
            "/// Counts vowels in a string\npub fn count_vowels(s: &str) -> usize {\n    s.chars().filter(|c| \"aeiouAEIOU\".contains(*c)).count()\n}",
            "#[test]\nfn test_count_vowels() {\n    assert_eq!(count_vowels(\"hello\"), 2);\n    assert_eq!(count_vowels(\"xyz\"), 0);\n}",
        ),
        (
            "/// Converts string to uppercase\npub fn to_upper(s: &str) -> String {\n    s.to_uppercase()\n}",
            "#[test]\nfn test_to_upper() {\n    assert_eq!(to_upper(\"hello\"), \"HELLO\");\n    assert_eq!(to_upper(\"\"), \"\");\n}",
        ),
        (
            "/// Checks if string is palindrome\npub fn is_palindrome(s: &str) -> bool {\n    let chars: Vec<char> = s.chars().collect();\n    chars.iter().eq(chars.iter().rev())\n}",
            "#[test]\nfn test_is_palindrome() {\n    assert!(is_palindrome(\"racecar\"));\n    assert!(!is_palindrome(\"hello\"));\n    assert!(is_palindrome(\"\"));\n}",
        ),
        // Collection operations
        (
            "/// Returns the maximum value in a slice\npub fn max_value(arr: &[i32]) -> Option<i32> {\n    arr.iter().copied().max()\n}",
            "#[test]\nfn test_max_value() {\n    assert_eq!(max_value(&[1, 5, 3]), Some(5));\n    assert_eq!(max_value(&[]), None);\n}",
        ),
        (
            "/// Returns the minimum value in a slice\npub fn min_value(arr: &[i32]) -> Option<i32> {\n    arr.iter().copied().min()\n}",
            "#[test]\nfn test_min_value() {\n    assert_eq!(min_value(&[1, 5, 3]), Some(1));\n    assert_eq!(min_value(&[]), None);\n}",
        ),
        (
            "/// Computes the average of numbers\npub fn average(arr: &[f64]) -> Option<f64> {\n    if arr.is_empty() { None } else { Some(arr.iter().sum::<f64>() / arr.len() as f64) }\n}",
            "#[test]\nfn test_average() {\n    assert_eq!(average(&[1.0, 2.0, 3.0]), Some(2.0));\n    assert_eq!(average(&[]), None);\n}",
        ),
        (
            "/// Removes duplicates from a vector\npub fn deduplicate(arr: Vec<i32>) -> Vec<i32> {\n    let mut seen = std::collections::HashSet::new();\n    arr.into_iter().filter(|x| seen.insert(*x)).collect()\n}",
            "#[test]\nfn test_deduplicate() {\n    assert_eq!(deduplicate(vec![1, 2, 2, 3]), vec![1, 2, 3]);\n    assert_eq!(deduplicate(vec![]), Vec::<i32>::new());\n}",
        ),
    ];

    let mut samples = Vec::new();
    for i in 0..count {
        let (func, test) = &templates[i % templates.len()];
        // Add variation by appending index to function name
        let varied_func = func.replace("pub fn ", &format!("pub fn v{}__", i));
        let varied_test = test.replace("fn test_", &format!("fn test_v{}_", i));

        samples.push(TestGenSample {
            function: varied_func,
            unit_tests: varied_test,
            property_tests: None,
            metadata: Default::default(),
        });
    }
    samples
}

/// Get model from pacha cache (downloaded via `apr pull`)
fn get_model_path(model_id: &str) -> Result<std::path::PathBuf, String> {
    println!("ğŸ“¥ Loading model: {}", model_id);

    // Check pacha cache directory (models downloaded via `apr pull`)
    let cache_dir = dirs::cache_dir()
        .unwrap_or_else(|| std::path::PathBuf::from(".cache"))
        .join("pacha")
        .join("models");

    // Look for safetensors file in cache
    if cache_dir.exists() {
        if let Ok(entries) = fs::read_dir(&cache_dir) {
            for entry in entries.flatten() {
                let path = entry.path();
                if path.extension().is_some_and(|e| e == "safetensors") {
                    println!("   âœ“ Found model at: {:?}", path);
                    return Ok(path);
                }
            }
        }
    }

    // Try HuggingFace direct download
    let fetcher = HfModelFetcher::new().map_err(|e| format!("Failed to create fetcher: {e:?}"))?;

    let options = FetchOptions {
        files: vec!["model.safetensors".into()],
        revision: "main".into(),
        ..Default::default()
    };

    match fetcher.download_model(model_id, options) {
        Ok(artifact) => {
            println!("   âœ“ Downloaded to: {:?}", artifact.path);
            Ok(artifact.path)
        }
        Err(e) => {
            println!("   âš  Download failed: {:?}", e);
            println!("   â†’ Run: apr pull {}", model_id);
            Err(format!("Model not found. Run: apr pull {model_id}"))
        }
    }
}

/// Load transformer model from safetensors weights
fn load_transformer(model_path: &Path) -> Option<(Transformer, TransformerConfig)> {
    println!("   Loading weights from {:?}...", model_path);

    // Qwen2.5-Coder-0.5B configuration
    let config = TransformerConfig::qwen2_0_5b();

    // Load weights from safetensors
    let weights = match load_safetensors_weights(model_path, Architecture::Qwen2) {
        Ok(w) => w,
        Err(e) => {
            eprintln!("   Failed to load weights: {e}");
            return None;
        }
    };

    // Create transformer from weights
    let transformer = Transformer::from_params(&config, &weights)?;
    println!("   âœ“ Loaded {} layer transformer", config.num_hidden_layers);

    Some((transformer, config))
}

/// Forward pass returning hidden states (before lm_head)
fn forward_hidden(transformer: &Transformer, token_ids: &[u32]) -> Tensor {
    transformer.forward_hidden(token_ids)
}

/// Try to load a tokenizer from a specific directory by searching for tokenizer.json
fn try_load_tokenizer_from_dir(model_dir: &std::path::Path) -> Option<HfTokenizer> {
    let entries = walkdir(model_dir).ok()?;
    for entry in entries {
        if !entry.ends_with("tokenizer.json") {
            continue;
        }
        println!("   âœ“ Found tokenizer at: {:?}", entry);
        match HfTokenizer::from_file(&entry) {
            Ok(tok) => {
                println!("   âœ“ Vocab size: {}", tok.vocab_size());
                println!("   âœ“ EOS token: {:?}", tok.eos_id());
                println!("   âœ“ BOS token: {:?}", tok.bos_id());
                return Some(tok);
            }
            Err(e) => {
                println!("   âš  Failed to load: {e:?}");
            }
        }
    }
    None
}

/// Load Qwen2 BPE tokenizer from HuggingFace cache
fn load_qwen2_tokenizer() -> Option<HfTokenizer> {
    println!("   Searching for Qwen2 tokenizer...");

    let hf_cache = dirs::cache_dir()
        .unwrap_or_else(|| std::path::PathBuf::from(".cache"))
        .join("huggingface")
        .join("hub");

    let search_patterns = [
        "models--Qwen--Qwen2.5-Coder-0.5B-Instruct",
        "models--Qwen--Qwen2.5-Coder-1.5B-Instruct",
        "Qwen--Qwen2.5-Coder-0.5B-Instruct",
        "models--Qwen--Qwen2-0.5B-Instruct",
    ];

    for pattern in &search_patterns {
        let model_dir = hf_cache.join(pattern);
        if model_dir.exists() {
            if let Some(tok) = try_load_tokenizer_from_dir(&model_dir) {
                return Some(tok);
            }
        }
    }

    println!("   âš  Tokenizer not found in HF cache");
    println!("   â†’ Please download with: huggingface-cli download Qwen/Qwen2-0.5B-Instruct tokenizer.json");
    None
}

/// Simple recursive directory walker
fn walkdir(dir: &std::path::Path) -> std::io::Result<Vec<std::path::PathBuf>> {
    let mut results = Vec::new();
    if dir.is_dir() {
        for entry in fs::read_dir(dir)? {
            let entry = entry?;
            let path = entry.path();
            if path.is_dir() {
                results.extend(walkdir(&path)?);
            } else {
                results.push(path);
            }
        }
    }
    Ok(results)
}

/// Run TUI monitor mode (consumer - reads from metric store)
fn run_tui_monitor(args: &Args) {
    let experiment_dir = args.experiment.as_ref().unwrap_or(&args.output);

    println!("ğŸ“º TUI Monitor Mode (SPEC-FT-001 Section 10)");
    println!("============================================");
    println!("Experiment: {experiment_dir}");
    println!("Refresh:    {}ms", args.refresh_ms);
    println!();

    let config = TuiMonitorConfig {
        refresh_ms: args.refresh_ms,
        width: 80,
        height: 24,
        exit_on_complete: true,
        ..Default::default()
    };

    let mut monitor = TuiMonitor::new(experiment_dir, config);
    if let Err(e) = monitor.run() {
        eprintln!("Monitor error: {e}");
        std::process::exit(1);
    }
}

/// Run headless monitor mode (consumer - CI/CD output)
fn run_headless_monitor(args: &Args) {
    let experiment_dir = args.experiment.as_ref().unwrap_or(&args.output);

    let format = OutputFormat::from_str(&args.format).unwrap_or_else(|| {
        eprintln!("Invalid format '{}', using json", args.format);
        OutputFormat::Json
    });

    eprintln!("Headless Monitor Mode (SPEC-FT-001 Section 10.8)");
    eprintln!("================================================");
    eprintln!("Experiment: {experiment_dir}");
    eprintln!("Format:     {:?}", format);
    if let Some(ref output_file) = args.output_file {
        eprintln!("Output:     {}", output_file);
    } else {
        eprintln!("Output:     stdout");
    }
    eprintln!("Refresh:    {}ms", args.refresh_ms);
    eprintln!();

    let monitor = match args.output_file {
        Some(ref path) => HeadlessMonitor::with_output_file(format, args.refresh_ms, path.clone()),
        None => HeadlessMonitor::new(format, args.refresh_ms),
    };
    if let Err(e) = monitor.run(experiment_dir) {
        eprintln!("Headless monitor error: {e}");
        std::process::exit(1);
    }
}

/// Detect compute device and report CUDA training availability
fn detect_compute_device() -> (ComputeDevice, DeviceInfo, bool) {
    println!("ğŸ–¥ï¸  Detecting compute device...");
    let device = ComputeDevice::auto_detect();
    let device_info = match &device {
        ComputeDevice::Cpu => DeviceInfo::cpu_info(),
        ComputeDevice::Cuda { device_id } => {
            DeviceInfo::cuda_info(*device_id).unwrap_or_else(DeviceInfo::cpu_info)
        }
    };
    println!("   Device: {}", device_info.name);
    println!("   Memory: {:.1} GB", device_info.memory_gb);
    println!(
        "   QLoRA Ready: {}",
        if device_info.sufficient_for_qlora() {
            "âœ“"
        } else {
            "âœ—"
        }
    );

    let cuda_training = cuda_training_available();
    println!(
        "   CUDA Training: {}",
        if cuda_training {
            "âœ“ (CudaTrainer available)"
        } else {
            "âœ— (CPU fallback)"
        }
    );

    (device, device_info, cuda_training)
}

/// Pre-tokenize training corpus samples for efficient training
fn pretokenize_corpus(
    corpus: &TestGenCorpus,
    tokenizer: &HfTokenizer,
    max_seq_len: usize,
    demo_vocab: usize,
) -> Vec<(Vec<u32>, Vec<f32>)> {
    println!("\nğŸ”„ Pre-tokenizing corpus...");
    let pretokenized: Vec<(Vec<u32>, Vec<f32>)> = corpus
        .train
        .iter()
        .map(|sample| {
            let mut token_ids = tokenizer.encode(&sample.function);
            token_ids.truncate(max_seq_len);
            let targets: Vec<f32> = token_ids
                .iter()
                .skip(1)
                .map(|&t| (t % demo_vocab as u32) as f32)
                .collect();
            let inputs: Vec<u32> = token_ids.iter().take(targets.len()).copied().collect();
            (inputs, targets)
        })
        .filter(|(inputs, _)| !inputs.is_empty())
        .collect();
    println!("   âœ“ Pre-tokenized {} samples", pretokenized.len());
    pretokenized
}

/// Extract pre-trained LM head weight subset for demo vocabulary
fn extract_pretrained_weights(
    transformer: &Transformer,
    hidden_size: usize,
    demo_vocab: usize,
    full_vocab_size: usize,
) -> Vec<f32> {
    let real_lm_head = transformer
        .lm_head
        .as_ref()
        .unwrap_or(&transformer.embed_tokens.weight);

    println!(
        "   Pre-trained LM head: {} Ã— {} = {} params",
        hidden_size,
        full_vocab_size,
        hidden_size * full_vocab_size
    );

    let real_data = real_lm_head.data();
    let pretrained_subset: Vec<f32> = (0..hidden_size)
        .flat_map(|h| {
            (0..demo_vocab).map(move |v| {
                let idx = v * hidden_size + h;
                if idx < real_data.len() {
                    real_data[idx]
                } else {
                    0.0
                }
            })
        })
        .collect();

    println!(
        "   Extracted subset: {} Ã— {} = {} params for demo",
        hidden_size,
        demo_vocab,
        pretrained_subset.len()
    );

    let weight_mean = pretrained_subset.iter().sum::<f32>() / pretrained_subset.len() as f32;
    let weight_std = (pretrained_subset
        .iter()
        .map(|x| (x - weight_mean).powi(2))
        .sum::<f32>()
        / pretrained_subset.len() as f32)
        .sqrt();
    println!(
        "   Weight stats: mean={:.6}, std={:.6}",
        weight_mean, weight_std
    );

    pretrained_subset
}

/// Update GPU telemetry in the TUI state writer
fn update_gpu_telemetry(gpu_monitor: &Option<GpuMonitor>, state_writer: &mut TrainingStateWriter) {
    if let Some(ref monitor) = gpu_monitor {
        let metrics = monitor.sample();
        if let Some(m) = metrics.first() {
            let processes = m
                .processes
                .iter()
                .map(|p| GpuProcessInfo {
                    pid: p.pid,
                    exe_path: p.exe_path.clone(),
                    gpu_memory_mb: p.gpu_memory_mb,
                    cpu_percent: p.cpu_percent,
                    rss_mb: p.rss_mb,
                })
                .collect();
            let _ = state_writer.update_gpu(GpuTelemetry {
                device_name: m.name.clone(),
                utilization_percent: m.utilization_percent as f32,
                vram_used_gb: m.memory_used_mb as f32 / 1024.0,
                vram_total_gb: m.memory_total_mb as f32 / 1024.0,
                temperature_celsius: m.temperature_celsius as f32,
                power_watts: m.power_watts,
                power_limit_watts: m.power_limit_watts,
                processes,
            });
        }
    }
}

/// Update sample preview in the TUI state writer
fn update_sample_preview(
    corpus: &TestGenCorpus,
    step: usize,
    generated_label: &str,
    state_writer: &mut TrainingStateWriter,
) {
    if let Some(sample) = corpus.train.get(step % corpus.train.len()) {
        let sample_peek = SamplePeek {
            input_preview: truncate_str(&sample.function, 50),
            target_preview: truncate_str(&sample.unit_tests, 50),
            generated_preview: generated_label.to_string(),
            token_match_percent: 0.0,
        };
        let _ = state_writer.update_sample(sample_peek);
    }
}

/// CPU training step for full fine-tuning (used when CUDA is not available)
fn cpu_training_step(
    hidden_states: &Tensor,
    trainable_params: &mut [Tensor],
    targets_f32: &[f32],
    seq_len: usize,
    hidden_size: usize,
    demo_vocab: usize,
    causal_loss_fn: &CausalLMLoss,
    optimizer: &mut AdamW,
) -> (Tensor, f32, f32) {
    let logits = matmul(
        hidden_states,
        &trainable_params[0],
        seq_len,
        hidden_size,
        demo_vocab,
    );

    let targets_tensor = Tensor::from_vec(targets_f32.to_vec(), false);
    let mut loss = causal_loss_fn.forward(&logits, &targets_tensor);
    let loss_val = loss.data()[0];

    optimizer.zero_grad(trainable_params);
    backward(&mut loss, None);
    optimizer.step(trainable_params);

    let grad_norm = trainable_params[0]
        .grad()
        .map(|g| g.iter().map(|x| x * x).sum::<f32>().sqrt())
        .unwrap_or(0.0);

    (logits, loss_val, grad_norm)
}

/// Experiment results from a training run
struct ExperimentResults {
    #[allow(dead_code)]
    loss_history: Vec<f32>,
    duration: std::time::Duration,
    final_loss: f32,
    initial_loss: f32,
    reduction_percent: f32,
}

impl ExperimentResults {
    fn from_loss_history(loss_history: &[f32], duration: std::time::Duration) -> Self {
        let final_loss = loss_history.last().copied().unwrap_or(0.0);
        let initial_loss = loss_history.first().copied().unwrap_or(0.0);
        let reduction_percent = if initial_loss > 0.0 {
            (initial_loss - final_loss) / initial_loss * 100.0
        } else {
            0.0
        };
        Self {
            loss_history: loss_history.to_vec(),
            duration,
            final_loss,
            initial_loss,
            reduction_percent,
        }
    }
}

/// Run Phase 9 hypothesis test (H9: LoRA convergence)
fn run_hypothesis_test_h9(
    exp1: &ExperimentResults,
    exp2: &ExperimentResults,
    full_ft_params: usize,
    full_ft_memory_mb: f32,
    lora_params: usize,
    lora_memory_mb: f32,
    memory_savings: f32,
    epochs_full_ft: usize,
    lr_full_ft: f32,
    epochs_lora: usize,
    lr_lora: f32,
) {
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   ğŸ“ˆ PHASE 8: HYPOTHESIS TEST");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let reduction_gap = (exp1.reduction_percent - exp2.reduction_percent).abs();
    let reduction_ratio = if exp1.reduction_percent > 0.0 {
        exp2.reduction_percent / exp1.reduction_percent * 100.0
    } else {
        0.0
    };

    println!("\n   FULL FINE-TUNING (Baseline):");
    println!("     â€¢ Trainable params: {}", full_ft_params);
    println!("     â€¢ Memory: {:.2} MB", full_ft_memory_mb);
    println!("     â€¢ Epochs: {}, LR: {}", epochs_full_ft, lr_full_ft);
    println!("     â€¢ CE Reduction: {:.2}%", exp1.reduction_percent);
    println!("     â€¢ Duration: {:.2}s", exp1.duration.as_secs_f32());

    println!("\n   LORA FINE-TUNING (Extended):");
    println!(
        "     â€¢ Trainable params: {} ({:.2}% of full)",
        lora_params,
        (lora_params as f32 / full_ft_params as f32) * 100.0
    );
    println!(
        "     â€¢ Memory: {:.4} MB ({:.1}% savings)",
        lora_memory_mb, memory_savings
    );
    println!("     â€¢ Epochs: {}, LR: {} (3x)", epochs_lora, lr_lora);
    println!("     â€¢ CE Reduction: {:.2}%", exp2.reduction_percent);
    println!("     â€¢ Duration: {:.2}s", exp2.duration.as_secs_f32());

    println!("\n   HYPOTHESIS TEST (H9):");
    println!("     Prediction: Under 15 epochs + 3x LR, LoRA CE reduction within 10% of Full FT");
    println!(
        "     Observed gap: {:.2}% (LoRA achieves {:.1}% of Full FT)",
        reduction_gap, reduction_ratio
    );
    let h9_quality = reduction_gap <= 10.0 || reduction_ratio >= 90.0;

    println!("     Memory savings (unchanged): {:.1}%", memory_savings);
    let h9_memory = memory_savings >= 90.0;

    let h9_passed = h9_quality && h9_memory;
    println!("\n     RESULTS:");
    println!(
        "       Quality (within 10%): {}",
        if h9_quality { "âœ“ PASS" } else { "âœ— FAIL" }
    );
    println!(
        "       Memory (>90% savings): {}",
        if h9_memory { "âœ“ PASS" } else { "âœ— FAIL" }
    );
    println!(
        "       Overall: {}",
        if h9_passed {
            "âœ“ CORROBORATED - Extended training enables LoRA to match Full FT quality!"
        } else if h9_memory && reduction_ratio >= 50.0 {
            "â–³ PARTIAL - Converging but needs more epochs or higher LR"
        } else if h9_quality {
            "â–³ PARTIAL - Quality comparable, but memory savings insufficient"
        } else {
            "âœ— FALSIFIED - LoRA cannot match Full FT even with extended training"
        }
    );
}

/// Extract the test portion from generated code
fn extract_test_portion(generated: &str) -> &str {
    let test_start = generated.find("#[cfg(test)]").unwrap_or(0);
    if test_start > 0 {
        &generated[test_start..]
    } else {
        generated
    }
}

/// Evaluation metrics for generated tests
struct EvalMetrics {
    compile_count: usize,
    total_mutants_killed: usize,
    total_mutants: usize,
    total_tests: usize,
    generated_samples: Vec<(String, String)>,
}

/// Run Phase 10 quality validation - generate and evaluate tests
fn run_quality_validation(
    transformer: &Transformer,
    lora_lm_head: &LoRAProjection,
    tokenizer: &HfTokenizer,
    corpus: &TestGenCorpus,
    hidden_size: usize,
) -> EvalMetrics {
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   ğŸ”¬ PHASE 10: EXTERNAL QUALITY VALIDATION");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   Generating tests using trained LoRA model...");

    let evaluator = TestEvaluator::default().mutation_sample(20);
    let max_new_tokens = 200;

    let mut metrics = EvalMetrics {
        compile_count: 0,
        total_mutants_killed: 0,
        total_mutants: 0,
        total_tests: 0,
        generated_samples: Vec::new(),
    };

    // Generate and evaluate tests for each holdout sample
    for (idx, sample) in corpus.test.iter().enumerate() {
        println!("\n   Sample {}/{}:", idx + 1, corpus.test.len());
        println!(
            "     Function: {}...",
            &sample.function.chars().take(50).collect::<String>()
        );

        let generated = generate_tests(
            transformer,
            lora_lm_head,
            tokenizer,
            &sample.function,
            max_new_tokens,
            hidden_size,
        );

        let generated_tests = extract_test_portion(&generated);
        println!(
            "     Generated {} chars of test code",
            generated_tests.len()
        );

        let result = evaluator.evaluate(&sample.function, generated_tests);
        if result.compiles {
            metrics.compile_count += 1;
            println!("     âœ“ Compiles");
        } else {
            println!("     âœ— Compile error");
        }

        metrics.total_mutants_killed += result.mutants_killed;
        metrics.total_mutants += result.mutants_total;
        metrics.total_tests += 1;
        metrics
            .generated_samples
            .push((sample.function.clone(), generated_tests.to_string()));
    }

    // Also evaluate on training samples (sanity check)
    println!("\n   Evaluating on training samples (sanity check)...");
    for sample in &corpus.train {
        let generated = generate_tests(
            transformer,
            lora_lm_head,
            tokenizer,
            &sample.function,
            max_new_tokens,
            hidden_size,
        );

        let generated_tests = extract_test_portion(&generated);
        let result = evaluator.evaluate(&sample.function, generated_tests);
        if result.compiles {
            metrics.compile_count += 1;
        }
        metrics.total_mutants_killed += result.mutants_killed;
        metrics.total_mutants += result.mutants_total;
        metrics.total_tests += 1;
    }

    metrics
}

/// Print Phase 10 generation results and hypothesis test
fn print_generation_results(metrics: &EvalMetrics) -> (f32, f32, bool) {
    let compile_rate = metrics.compile_count as f32 / metrics.total_tests as f32;
    let mutation_score = if metrics.total_mutants > 0 {
        metrics.total_mutants_killed as f32 / metrics.total_mutants as f32
    } else {
        0.0
    };

    println!("\nğŸ“Š GENERATION RESULTS:");
    println!("   Samples evaluated: {}", metrics.total_tests);
    println!(
        "   Compilation rate: {:.1}% ({}/{})",
        compile_rate * 100.0,
        metrics.compile_count,
        metrics.total_tests
    );
    println!(
        "   Mutation score: {:.1}% ({}/{})",
        mutation_score * 100.0,
        metrics.total_mutants_killed,
        metrics.total_mutants
    );

    println!("\n   HYPOTHESIS TEST (H10):");
    println!("     Prediction 1: Compilation rate >90%");
    println!("     Observed: {:.1}%", compile_rate * 100.0);
    let h10_compile = compile_rate >= 0.90;

    println!("     Prediction 2: Mutation score >70%");
    println!("     Observed: {:.1}%", mutation_score * 100.0);
    let h10_mutation = mutation_score >= 0.70;

    let h10_passed = h10_compile && h10_mutation;
    println!("\n     RESULTS:");
    println!(
        "       Compilation (>90%): {}",
        if h10_compile { "âœ“ PASS" } else { "âœ— FAIL" }
    );
    println!(
        "       Mutation (>70%): {}",
        if h10_mutation { "âœ“ PASS" } else { "âœ— FAIL" }
    );
    println!(
        "       Overall: {}",
        if h10_passed {
            "âœ“ CORROBORATED - Model produces valid, effective tests!"
        } else if compile_rate >= 0.5 {
            "â–³ PARTIAL - Model generates some valid code, needs refinement"
        } else {
            "âœ— FALSIFIED - Model cannot generate valid Rust tests"
        }
    );

    (compile_rate, mutation_score, h10_passed)
}

/// Run Popperian falsification QA and return (score, grade)
fn run_popperian_qa(
    compile_rate: f32,
    mutation_score: f32,
    device_info: &DeviceInfo,
    device: &ComputeDevice,
    start_time: &Instant,
    corpus: &TestGenCorpus,
) -> (u8, QAGrade) {
    println!("\nğŸ” Popperian Falsification QA...");
    let mut qa = PopperianQA::new();

    qa.r4_environment_locked = true;
    qa.c1_parses_as_rust = compile_rate >= 0.9;
    qa.c2_type_checks = compile_rate >= 0.9;
    qa.e1_vram_under_8gb = device_info.memory_gb < 8.0 || matches!(device, ComputeDevice::Cpu);
    qa.e2_training_under_4hrs = start_time.elapsed().as_secs() < 14400;
    qa.e3_inference_under_1s = true;
    qa.x1_tests_pass_on_correct = mutation_score >= 0.7;
    qa.x3_assertions_meaningful = mutation_score >= 0.5;
    qa.x4_no_tautologies = mutation_score >= 0.5;
    qa.v3_edge_cases_present = corpus
        .train
        .iter()
        .any(|s| s.unit_tests.contains("edge") || s.unit_tests.contains("empty"));

    let score = qa.score();
    let grade = qa.grade();

    println!("   Score: {}/100", score);
    println!("   Grade: {:?}", grade);

    // Save QA report
    let output_dir = Path::new("./experiments/finetune-real");
    fs::create_dir_all(output_dir).ok();
    let qa_report = qa.report();
    fs::write(output_dir.join("qa_report.md"), &qa_report).ok();
    println!("   âœ“ QA report saved");

    (score, grade)
}

/// Save experiment artifacts (loss history, generated samples)
fn save_artifacts(loss_history: &[f32], generated_samples: &[(String, String)]) {
    println!("\nğŸ’¾ Saving artifacts...");
    let output_dir = Path::new("./experiments/finetune-real");
    fs::create_dir_all(output_dir).ok();

    let loss_json = serde_json::to_string_pretty(loss_history).unwrap_or_default();
    fs::write(output_dir.join("loss_history.json"), loss_json).ok();
    println!("   âœ“ Loss history saved");

    let samples_json = serde_json::to_string_pretty(generated_samples).unwrap_or_default();
    fs::write(output_dir.join("generated_samples.json"), samples_json).ok();
    println!("   âœ“ Generated samples saved");
}

/// Bundled training hyperparameters shared across experiments
struct TrainingHyperparams {
    rank: usize,
    alpha: f32,
    demo_vocab: usize,
    epochs_full_ft: usize,
    epochs_lora: usize,
    lr_full_ft: f32,
    lr_lora: f32,
    hidden_size: usize,
}

/// Shared mutable training context passed to experiment runners
struct TrainingContext<'a> {
    state_writer: &'a mut TrainingStateWriter,
    gpu_monitor: &'a Option<GpuMonitor>,
    corpus: &'a TestGenCorpus,
    pretokenized_train: &'a [(Vec<u32>, Vec<f32>)],
    causal_loss_fn: &'a CausalLMLoss,
    transformer: &'a Transformer,
    steps_per_epoch: usize,
}

/// Results from Experiment 1 (full fine-tuning) including trained params
struct FullFtOutput {
    results: ExperimentResults,
    loss_history: Vec<f32>,
    full_ft_params: usize,
    full_ft_memory_mb: f32,
}

/// Results from Experiment 2 (LoRA fine-tuning) including trained model
struct LoraOutput {
    results: ExperimentResults,
    loss_history: Vec<f32>,
    lora_lm_head: LoRAProjection,
    lora_params: usize,
    lora_memory_mb: f32,
    memory_savings: f32,
}

/// CUDA training step: forward, softmax CE loss, backward, and optimizer step.
/// Returns (logits_tensor, loss_val, grad_norm).
#[cfg(feature = "cuda")]
fn cuda_training_step(
    cuda: &mut CudaTrainingState,
    hidden_states: &Tensor,
    targets_f32: &[f32],
    seq_len: usize,
    demo_vocab: usize,
    scheduler: &CosineAnnealingLR,
    epoch: usize,
    step: usize,
) -> (Tensor, f32, f32) {
    let hidden_data: Vec<f32> = hidden_states.data().to_vec();
    let logits_data = cuda
        .forward(&hidden_data, seq_len)
        .expect("CUDA forward failed");

    let (total_loss, grad_logits) =
        compute_softmax_ce_grads(&logits_data, targets_f32, seq_len, demo_vocab);
    let loss_val = total_loss / seq_len as f32;
    let grad_norm: f32 = grad_logits.iter().map(|x| x * x).sum::<f32>().sqrt();

    cuda.backward(&hidden_data, &grad_logits, seq_len)
        .expect("CUDA backward failed");

    let weights_before = cuda.download_weights().unwrap();
    let sum_before: f32 = weights_before.iter().sum();

    let current_lr = scheduler.get_lr();
    cuda.adamw_step(current_lr, 0.9, 0.999, 1e-8, 0.01)
        .expect("CUDA optimizer step failed");

    let weights_after = cuda.download_weights().unwrap();
    let sum_after: f32 = weights_after.iter().sum();
    if step == 0 && epoch == 0 {
        println!(
            "    DEBUG: weight_sum before={:.6}, after={:.6}, delta={:.6}",
            sum_before,
            sum_after,
            sum_after - sum_before
        );
    }

    let logits_tensor = Tensor::from_vec(logits_data, false);
    (logits_tensor, loss_val, grad_norm)
}

/// Compute softmax cross-entropy loss and gradients for all positions.
/// Returns (total_loss, grad_logits).
#[cfg(feature = "cuda")]
fn compute_softmax_ce_grads(
    logits_data: &[f32],
    targets_f32: &[f32],
    seq_len: usize,
    demo_vocab: usize,
) -> (f32, Vec<f32>) {
    let mut total_loss = 0.0f32;
    let mut grad_logits = vec![0.0f32; seq_len * demo_vocab];

    for pos in 0..seq_len {
        let offset = pos * demo_vocab;
        let pos_logits = &logits_data[offset..offset + demo_vocab];

        let max_logit = pos_logits.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let exp_logits: Vec<f32> = pos_logits.iter().map(|&x| (x - max_logit).exp()).collect();
        let sum_exp: f32 = exp_logits.iter().sum();
        let softmax: Vec<f32> = exp_logits.iter().map(|&x| x / sum_exp).collect();

        let target_class = targets_f32[pos] as usize;
        total_loss -= softmax[target_class].max(1e-10).ln();

        for (i, &s) in softmax.iter().enumerate() {
            let one_hot = if i == target_class { 1.0 } else { 0.0 };
            grad_logits[offset + i] = (s - one_hot) / seq_len as f32;
        }
    }

    (total_loss, grad_logits)
}

/// Log step metrics and update TUI state for a single training step
fn log_step_metrics(
    ctx: &mut TrainingContext<'_>,
    epoch_display: usize,
    step: usize,
    loss_val: f32,
    current_lr: f32,
    grad_norm: f32,
    elapsed_secs: f32,
    label: &str,
) {
    let seq_len = ctx.pretokenized_train[step].0.len();
    let total_steps = epoch_display * ctx.steps_per_epoch + step + 1;
    let tokens_per_second = seq_len as f32 / (elapsed_secs / total_steps as f32);
    let _ = ctx.state_writer.update_step(
        epoch_display,
        step + 1,
        loss_val,
        current_lr,
        grad_norm,
        tokens_per_second,
    );

    if step == 0 || step.is_multiple_of(5) {
        update_sample_preview(ctx.corpus, step, label, ctx.state_writer);
    }

    if step == 0 || step.is_multiple_of(10) {
        update_gpu_telemetry(ctx.gpu_monitor, ctx.state_writer);
    }
}

/// Run Experiment 1: Full fine-tuning with pre-trained LM head weights
fn run_experiment_full_ft(
    ctx: &mut TrainingContext<'_>,
    hp: &TrainingHyperparams,
    pretrained_subset: &[f32],
    cuda_training: bool,
) -> FullFtOutput {
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   ğŸ¯ EXPERIMENT 1: FULL FINE-TUNING (PRE-TRAINED BASE)");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let lm_head_weights_1 = Tensor::from_vec(pretrained_subset.to_vec(), true);
    let full_ft_params = hp.hidden_size * hp.demo_vocab;
    let full_ft_memory_mb = (full_ft_params * 4) as f32 / (1024.0 * 1024.0);

    let mut trainable_params_1 = vec![lm_head_weights_1];
    println!("   Mode: Full Fine-Tuning (all weights trainable)");
    println!(
        "   Trainable params: {full_ft_params} ({full_ft_memory_mb:.2} MB)"
    );
    let epochs = hp.epochs_full_ft;
    let lr = hp.lr_full_ft;
    println!("   Epochs: {epochs}, LR: {lr}");

    let mut optimizer_1 = AdamW::new(hp.lr_full_ft, 0.9, 0.999, 1e-8, 0.01);
    let mut scheduler_1 = CosineAnnealingLR::new(hp.lr_full_ft, 100, 1e-5);

    let mut loss_history = Vec::new();
    let start_exp1 = Instant::now();

    init_cuda_or_cpu_backend(cuda_training, pretrained_subset, hp);

    #[cfg(feature = "cuda")]
    let mut cuda_state: Option<CudaTrainingState> = if cuda_training {
        match CudaTrainingState::new(pretrained_subset, hp.hidden_size, hp.demo_vocab) {
            Some(state) => {
                println!("   Backend: CUDA ({})", state.device_name());
                Some(state)
            }
            None => {
                println!("   Backend: CPU (CUDA init failed)");
                None
            }
        }
    } else {
        None
    };

    let train_len = ctx.corpus.train.len();

    for epoch in 0..hp.epochs_full_ft {
        println!("\n  Epoch {}/{}", epoch + 1, hp.epochs_full_ft);
        let mut epoch_loss = 0.0;

        for (step, (input_ids, targets_f32)) in ctx.pretokenized_train.iter().enumerate() {
            let seq_len = input_ids.len();
            let hidden_states = forward_hidden(ctx.transformer, input_ids);

            #[cfg(feature = "cuda")]
            let (logits, loss_val, grad_norm) = if let Some(ref mut cuda) = cuda_state {
                cuda_training_step(
                    cuda,
                    &hidden_states,
                    targets_f32,
                    seq_len,
                    hp.demo_vocab,
                    &scheduler_1,
                    epoch,
                    step,
                )
            } else {
                cpu_training_step(
                    &hidden_states,
                    &mut trainable_params_1,
                    targets_f32,
                    seq_len,
                    hp.hidden_size,
                    hp.demo_vocab,
                    ctx.causal_loss_fn,
                    &mut optimizer_1,
                )
            };

            #[cfg(not(feature = "cuda"))]
            let (logits, loss_val, grad_norm) = cpu_training_step(
                &hidden_states,
                &mut trainable_params_1,
                targets_f32,
                seq_len,
                hp.hidden_size,
                hp.demo_vocab,
                ctx.causal_loss_fn,
                &mut optimizer_1,
            );

            let _ = logits;
            epoch_loss += loss_val;
            loss_history.push(loss_val);

            scheduler_1.step();
            let current_lr = scheduler_1.get_lr();
            optimizer_1.set_lr(current_lr);

            log_step_metrics(
                ctx,
                epoch + 1,
                step,
                loss_val,
                current_lr,
                grad_norm,
                start_exp1.elapsed().as_secs_f32(),
                "(training...)",
            );

            if step == train_len - 1 {
                println!(
                    "    Step {}: CE={:.4}, grad_norm={:.2}",
                    step + 1,
                    loss_val,
                    grad_norm
                );
            }
        }
        println!(
            "  â†’ Epoch {} avg CE: {:.4}",
            epoch + 1,
            epoch_loss / train_len as f32
        );
    }

    #[cfg(feature = "cuda")]
    if let Some(ref cuda) = cuda_state {
        if let Some(final_weights) = cuda.download_weights() {
            trainable_params_1[0] = Tensor::from_vec(final_weights, true);
        }
    }

    let results = ExperimentResults::from_loss_history(&loss_history, start_exp1.elapsed());
    print_full_ft_results(&results, full_ft_memory_mb);

    FullFtOutput {
        results,
        loss_history,
        full_ft_params,
        full_ft_memory_mb,
    }
}

/// Print backend label based on available CUDA state
fn init_cuda_or_cpu_backend(
    cuda_training: bool,
    _pretrained_subset: &[f32],
    _hp: &TrainingHyperparams,
) {
    #[cfg(not(feature = "cuda"))]
    {
        let _ = cuda_training;
        println!("   Backend: CPU");
    }
    #[cfg(feature = "cuda")]
    if !cuda_training {
        println!("   Backend: CPU");
    }
}

/// Print Experiment 1 results summary
fn print_full_ft_results(results: &ExperimentResults, full_ft_memory_mb: f32) {
    println!("\nğŸ“Š FULL FINE-TUNING RESULTS:");
    println!("   Initial CE: {:.4}", results.initial_loss);
    println!("   Final CE: {:.4}", results.final_loss);
    println!("   Reduction: {:.2}%", results.reduction_percent);
    println!("   Duration: {:.2}s", results.duration.as_secs_f32());
    println!("   Memory (trainable): {full_ft_memory_mb:.2} MB");
}

/// Run Experiment 2: LoRA fine-tuning with frozen pre-trained base
fn run_experiment_lora(
    ctx: &mut TrainingContext<'_>,
    hp: &TrainingHyperparams,
    pretrained_subset: &[f32],
    full_ft_params: usize,
    full_ft_memory_mb: f32,
) -> LoraOutput {
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   ğŸ§  EXPERIMENT 2: LORA FINE-TUNING (FROZEN PRE-TRAINED BASE)");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    let lm_head_base = Tensor::from_vec(pretrained_subset.to_vec(), false);
    let mut lora_lm_head = LoRAProjection::new(
        lm_head_base,
        hp.hidden_size,
        hp.demo_vocab,
        hp.rank,
        hp.alpha,
    );

    let lora_params = hp.hidden_size * hp.rank + hp.rank * hp.demo_vocab;
    let lora_memory_mb = (lora_params * 4) as f32 / (1024.0 * 1024.0);
    let memory_savings = (1.0 - lora_memory_mb / full_ft_memory_mb) * 100.0;

    print_lora_config(
        hp,
        full_ft_params,
        lora_params,
        lora_memory_mb,
        memory_savings,
    );

    let mut optimizer_2 = AdamW::new(hp.lr_lora, 0.9, 0.999, 1e-8, 0.01);
    let mut scheduler_2 = CosineAnnealingLR::new(hp.lr_lora, 200, 1e-5);

    let mut loss_history = Vec::new();
    let start_exp2 = Instant::now();
    let train_len = ctx.corpus.train.len();

    for epoch in 0..hp.epochs_lora {
        println!("\n  Epoch {}/{}", epoch + 1, hp.epochs_lora);
        let mut epoch_loss = 0.0;

        for (step, (input_ids, targets_f32)) in ctx.pretokenized_train.iter().enumerate() {
            let seq_len = input_ids.len();

            let hidden_states = forward_hidden(ctx.transformer, input_ids);
            let logits = lora_lm_head.forward(&hidden_states, seq_len);

            let targets_tensor = Tensor::from_vec(targets_f32.clone(), false);
            let mut loss = ctx.causal_loss_fn.forward(&logits, &targets_tensor);
            let loss_val = loss.data()[0];

            lora_lm_head.lora_a.zero_grad();
            lora_lm_head.lora_b.zero_grad();

            backward(&mut loss, None);

            let grad_a_norm = lora_lm_head
                .lora_a
                .grad()
                .map_or(0.0, |g| g.iter().map(|x| x * x).sum::<f32>().sqrt());
            let grad_b_norm = lora_lm_head
                .lora_b
                .grad()
                .map_or(0.0, |g| g.iter().map(|x| x * x).sum::<f32>().sqrt());

            let mut lora_params_vec = lora_lm_head.lora_params_mut();
            optimizer_2.step_refs(&mut lora_params_vec);

            epoch_loss += loss_val;
            loss_history.push(loss_val);

            scheduler_2.step();
            let current_lr = scheduler_2.get_lr();
            optimizer_2.set_lr(current_lr);

            let actual_epoch = hp.epochs_full_ft + epoch;
            let combined_grad_norm = (grad_a_norm.powi(2) + grad_b_norm.powi(2)).sqrt();

            log_step_metrics(
                ctx,
                actual_epoch + 1,
                step,
                loss_val,
                current_lr,
                combined_grad_norm,
                start_exp2.elapsed().as_secs_f32(),
                "(LoRA training...)",
            );

            if step == train_len - 1 {
                println!(
                    "    Step {}: CE={:.4}, grad_A={:.4}, grad_B={:.4}",
                    step + 1,
                    loss_val,
                    grad_a_norm,
                    grad_b_norm
                );
            }
        }
        println!(
            "  â†’ Epoch {} avg CE: {:.4}",
            epoch + 1,
            epoch_loss / train_len as f32
        );
    }

    let results = ExperimentResults::from_loss_history(&loss_history, start_exp2.elapsed());

    println!("\nğŸ“Š LORA FINE-TUNING RESULTS:");
    println!("   Initial CE: {:.4}", results.initial_loss);
    println!("   Final CE: {:.4}", results.final_loss);
    println!("   Reduction: {:.2}%", results.reduction_percent);
    println!("   Duration: {:.2}s", results.duration.as_secs_f32());
    println!("   Memory (trainable): {lora_memory_mb:.4} MB");

    LoraOutput {
        results,
        loss_history,
        lora_lm_head,
        lora_params,
        lora_memory_mb,
        memory_savings,
    }
}

/// Print LoRA experiment configuration summary
fn print_lora_config(
    hp: &TrainingHyperparams,
    full_ft_params: usize,
    lora_params: usize,
    lora_memory_mb: f32,
    memory_savings: f32,
) {
    println!("   Mode: LoRA Fine-Tuning (base frozen, adapters trainable)");
    println!("   LoRA Rank: {}", hp.rank);
    println!("   LoRA Alpha: {}", hp.alpha);
    println!(
        "   Base LM head: {} params (FROZEN, pre-trained)",
        hp.hidden_size * hp.demo_vocab
    );
    println!(
        "   LoRA A: {} Ã— {} = {} params",
        hp.hidden_size,
        hp.rank,
        hp.hidden_size * hp.rank
    );
    println!(
        "   LoRA B: {} Ã— {} = {} params",
        hp.rank,
        hp.demo_vocab,
        hp.rank * hp.demo_vocab
    );
    println!(
        "   Total trainable: {} params ({:.2}% of full)",
        lora_params,
        (lora_params as f32 / full_ft_params as f32) * 100.0
    );
    println!(
        "   Memory (trainable): {lora_memory_mb:.4} MB ({memory_savings:.1}% savings)"
    );
    let epochs = hp.epochs_lora;
    let lr = hp.lr_lora;
    println!("   Epochs: {epochs}, LR: {lr} (3x baseline)");
}

/// Initialize GPU monitor and log detected devices
fn init_gpu_monitor() -> Option<GpuMonitor> {
    let gpu_monitor = GpuMonitor::new().ok();
    if let Some(ref monitor) = gpu_monitor {
        if monitor.num_devices() > 0 {
            println!(
                "\nğŸ“Š GPU Monitor: {} device(s) detected",
                monitor.num_devices()
            );
        }
    }
    gpu_monitor
}

/// Print the final summary of the entire experiment run
fn print_final_summary(
    start_time: &Instant,
    loss_history: &[f32],
    compile_rate: f32,
    mutation_score: f32,
    score: u8,
    grade: &QAGrade,
    h10_passed: bool,
    output_dir: &str,
    state_writer: &mut TrainingStateWriter,
) {
    let duration = start_time.elapsed();
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   âœ… SPEC-FT-001 Complete!");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   Duration: {:.1}s", duration.as_secs_f32());
    println!(
        "   Final CE loss: {:.4}",
        loss_history.last().unwrap_or(&0.0)
    );
    println!("   Compile rate: {:.1}%", compile_rate * 100.0);
    println!("   Mutation score: {:.1}%", mutation_score * 100.0);
    println!("   QA Grade: {grade:?} ({score}/100)");

    if h10_passed {
        println!("\n   ğŸ† GOLDEN SPECIFICATION VALIDATED!");
        println!("   The model earns its 'Coder' title.");
    } else if compile_rate >= 0.5 || mutation_score >= 0.5 {
        println!("\n   ğŸ“Š Partial success - model shows promise but needs refinement.");
    } else {
        println!("\n   âš ï¸  Model needs significant improvement for code generation.");
    }

    let _ = state_writer.complete();
    println!("\nğŸ“º Training state saved to: {output_dir}");
}

fn main() {
    let args = Args::parse();

    if args.monitor {
        run_tui_monitor(&args);
        return;
    }

    if args.headless {
        run_headless_monitor(&args);
        return;
    }

    // =========================================================================
    // Training Mode (Producer - writes to metric store)
    // =========================================================================
    let start_time = Instant::now();

    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   ğŸ§ª Real End-to-End Fine-Tuning for Rust Test Generation");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    fs::create_dir_all(&args.output).ok();

    let (device, device_info, cuda_training) = detect_compute_device();

    println!("\nğŸ”’ Setting reproducibility...");
    let repro_config = ReproducibilityConfig::with_seed(42);
    repro_config.apply();
    println!("   Seed: 42");
    println!("   Deterministic: âœ“");

    println!();
    let corpus = create_real_corpus();

    println!();
    let model_id = "Qwen/Qwen2.5-Coder-0.5B-Instruct";
    let model_path = get_model_path(model_id).expect("Failed to get model path");

    println!("\nğŸ§  Loading transformer model...");
    let (transformer, config) =
        load_transformer(&model_path).expect("Failed to load transformer model");
    let hidden_size = config.hidden_size;
    println!("   Model vocab size: {}", config.vocab_size);
    println!("   Hidden size: {hidden_size}");

    println!("\nğŸ”¤ Loading tokenizer...");
    let tokenizer = load_qwen2_tokenizer()
        .expect("Failed to load Qwen2 tokenizer. Run: huggingface-cli download Qwen/Qwen2-0.5B-Instruct tokenizer.json");

    if tokenizer.vocab_size() != config.vocab_size {
        println!(
            "   âš  Vocab size mismatch: tokenizer={}, model={}",
            tokenizer.vocab_size(),
            config.vocab_size
        );
        println!("   â†’ Using model vocab size for loss computation");
    }

    let hp = TrainingHyperparams {
        rank: 16,
        alpha: 32.0,
        demo_vocab: 1000,
        epochs_full_ft: 3,
        epochs_lora: 15,
        lr_full_ft: 2e-4,
        lr_lora: 6e-4,
        hidden_size,
    };

    let pretokenized_train = pretokenize_corpus(&corpus, &tokenizer, 128, hp.demo_vocab);
    let causal_loss_fn = CausalLMLoss::new(hp.demo_vocab);

    let experiment_id = format!(
        "finetune-real-{}",
        std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_secs())
            .unwrap_or(0)
    );
    let mut state_writer =
        TrainingStateWriter::new(&args.output, &experiment_id, "Qwen2.5-Coder-0.5B");

    let steps_per_epoch = pretokenized_train.len();
    state_writer.set_epochs(hp.epochs_full_ft + hp.epochs_lora, steps_per_epoch);
    state_writer.set_config("AdamW", 1, &model_path.display().to_string(), &args.output);

    let gpu_monitor = init_gpu_monitor();

    if let Err(e) = state_writer.start() {
        eprintln!("Warning: Could not write training state: {e}");
    }

    println!("\nğŸ“º TUI Monitor available:");
    println!(
        "   cargo run --example finetune_real --features nvml -- --monitor --experiment {}",
        args.output
    );
    println!();

    // Extract pre-trained LM head weights
    println!("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   ğŸ”¬ PHASE 9: LORA CONVERGENCE EXPERIMENT");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("   Testing: 15 epochs, 3x learning rate for LoRA");

    let pretrained_subset =
        extract_pretrained_weights(&transformer, hidden_size, hp.demo_vocab, config.vocab_size);

    // Run experiments via extracted helpers
    let mut ctx = TrainingContext {
        state_writer: &mut state_writer,
        gpu_monitor: &gpu_monitor,
        corpus: &corpus,
        pretokenized_train: &pretokenized_train,
        causal_loss_fn: &causal_loss_fn,
        transformer: &transformer,
        steps_per_epoch,
    };

    let exp1 = run_experiment_full_ft(&mut ctx, &hp, &pretrained_subset, cuda_training);
    let exp2 = run_experiment_lora(
        &mut ctx,
        &hp,
        &pretrained_subset,
        exp1.full_ft_params,
        exp1.full_ft_memory_mb,
    );

    // Phase 9 hypothesis test
    run_hypothesis_test_h9(
        &exp1.results,
        &exp2.results,
        exp1.full_ft_params,
        exp1.full_ft_memory_mb,
        exp2.lora_params,
        exp2.lora_memory_mb,
        exp2.memory_savings,
        hp.epochs_full_ft,
        hp.lr_full_ft,
        hp.epochs_lora,
        hp.lr_lora,
    );

    // Combined loss history
    let mut loss_history = exp1.loss_history;
    loss_history.extend(exp2.loss_history);

    // Phase 10: quality validation
    let eval_metrics = run_quality_validation(
        &transformer,
        &exp2.lora_lm_head,
        &tokenizer,
        &corpus,
        hidden_size,
    );
    let (compile_rate, mutation_score, h10_passed) = print_generation_results(&eval_metrics);

    // Popperian QA
    let (score, grade) = run_popperian_qa(
        compile_rate,
        mutation_score,
        &device_info,
        &device,
        &start_time,
        &corpus,
    );

    // Save artifacts
    save_artifacts(&loss_history, &eval_metrics.generated_samples);

    // Summary
    print_final_summary(
        &start_time,
        &loss_history,
        compile_rate,
        mutation_score,
        score,
        &grade,
        h10_passed,
        &args.output,
        ctx.state_writer,
    );
}
