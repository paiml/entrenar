# YAML-031: LoRA Fine-Tuning
# QA Focus: Trainable params < 1% of total
# Validation: 25-point checklist required

model:
  path: ./models/mock.gguf
  layers: [q_proj, v_proj]

data:
  train: ./data/train.parquet
  batch_size: 4

optimizer:
  name: adamw
  lr: 0.0001
  weight_decay: 0.01

lora:
  rank: 8
  alpha: 16.0
  target_modules: [q_proj, v_proj]
  dropout: 0.05

training:
  epochs: 3
  grad_clip: 1.0
  output_dir: ./outputs/lora
